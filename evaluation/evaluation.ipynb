{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Models Evaluation - Zero-Shot Voice Cloning\n",
    "\n",
    "Este notebook analiza las m√©tricas de rendimiento de los 3 modelos TTS implementados:\n",
    "- **YourTTS**: Modelo multiling√ºe general\n",
    "- **XTTS v2**: Modelo avanzado de alta calidad\n",
    "- **VITS**: Modelo ligero y r√°pido\n",
    "\n",
    "## M√©tricas Evaluadas\n",
    "\n",
    "1. **RTF (Real-Time Factor)**: Tiempo de generaci√≥n / Duraci√≥n del audio\n",
    "   - RTF < 1: M√°s r√°pido que tiempo real\n",
    "   - RTF = 1: Tiempo real\n",
    "   - RTF > 1: M√°s lento que tiempo real\n",
    "\n",
    "2. **Generation Time**: Tiempo total de inferencia\n",
    "\n",
    "3. **Audio Duration**: Duraci√≥n del audio generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Importaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los archivos JSON de resultados\n",
    "results_dir = Path('../results')\n",
    "json_files = sorted(results_dir.glob('metrics_*.json'))\n",
    "\n",
    "print(f\"Found {len(json_files)} result files:\")\n",
    "for f in json_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar y procesar los datos\n",
    "def load_metrics_from_json(json_path):\n",
    "    \"\"\"\n",
    "    Carga m√©tricas desde un archivo JSON.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con las m√©tricas de cada modelo\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extraer informaci√≥n general\n",
    "    timestamp = data.get('timestamp', 'unknown')\n",
    "    text = data.get('text', '')\n",
    "    reference = data.get('reference_audio', '')\n",
    "    \n",
    "    # Crear DataFrame con los resultados de cada modelo\n",
    "    models_data = []\n",
    "    for model in data.get('models', []):\n",
    "        if model.get('success', False):\n",
    "            models_data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'text': text,\n",
    "                'model': model['model'],\n",
    "                'audio_duration': model['audio_duration'],\n",
    "                'generation_time': model['generation_time'],\n",
    "                'rtf': model['rtf'],\n",
    "                'output_path': model['output_path'],\n",
    "                'success': True\n",
    "            })\n",
    "        else:\n",
    "            models_data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'text': text,\n",
    "                'model': model['model'],\n",
    "                'error': model.get('error', 'Unknown error'),\n",
    "                'success': False\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(models_data)\n",
    "\n",
    "# Cargar todos los resultados\n",
    "all_results = []\n",
    "for json_file in json_files:\n",
    "    df = load_metrics_from_json(json_file)\n",
    "    all_results.append(df)\n",
    "\n",
    "# Combinar todos los resultados\n",
    "if all_results:\n",
    "    metrics_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\nLoaded {len(metrics_df)} model evaluations\")\n",
    "    display(metrics_df.head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se encontraron archivos de m√©tricas.\")\n",
    "    print(\"Ejecuta primero: make run-all TEXT='Your test text'\")\n",
    "    metrics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estad√≠sticas Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty:\n",
    "    # Filtrar solo resultados exitosos\n",
    "    successful_df = metrics_df[metrics_df['success'] == True].copy()\n",
    "    \n",
    "    if not successful_df.empty:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ESTAD√çSTICAS POR MODELO\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Agrupar por modelo y calcular estad√≠sticas\n",
    "        stats = successful_df.groupby('model').agg({\n",
    "            'rtf': ['mean', 'std', 'min', 'max'],\n",
    "            'generation_time': ['mean', 'std', 'min', 'max'],\n",
    "            'audio_duration': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        \n",
    "        display(stats)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay resultados exitosos para analizar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparaci√≥n de RTF (Real-Time Factor)\n",
    "\n",
    "El RTF mide la eficiencia del modelo. Un RTF menor indica generaci√≥n m√°s r√°pida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty and 'rtf' in successful_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Crear gr√°fico de barras\n",
    "    models = successful_df.groupby('model')['rtf'].mean().sort_values()\n",
    "    colors = ['green' if rtf < 1 else 'orange' if rtf < 2 else 'red' for rtf in models]\n",
    "    \n",
    "    models.plot(kind='barh', ax=ax, color=colors)\n",
    "    \n",
    "    # A√±adir l√≠nea de referencia en RTF=1\n",
    "    ax.axvline(x=1, color='red', linestyle='--', linewidth=2, label='Real-time (RTF=1)')\n",
    "    \n",
    "    ax.set_xlabel('RTF (Real-Time Factor)', fontsize=12)\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    ax.set_title('Comparaci√≥n de Eficiencia: RTF por Modelo\\n(Menor es mejor)', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpretaci√≥n\n",
    "    fastest = models.idxmin()\n",
    "    print(f\"\\nüèÜ Modelo m√°s r√°pido: {fastest} (RTF: {models[fastest]:.2f}x)\")\n",
    "    \n",
    "    if models[fastest] < 1:\n",
    "        print(f\"   ‚Üí Genera audio {1/models[fastest]:.2f}x m√°s r√°pido que tiempo real\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Tarda {models[fastest]:.2f}x el tiempo del audio en generar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tiempo de Generaci√≥n Absoluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty and 'generation_time' in successful_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Box plot para mostrar distribuci√≥n\n",
    "    successful_df.boxplot(column='generation_time', by='model', ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "    ax.set_title('Distribuci√≥n de Tiempo de Generaci√≥n por Modelo', fontsize=14, fontweight='bold')\n",
    "    plt.suptitle('')  # Remove automatic title\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaci√≥n M√∫ltiple: RTF vs Generation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # RTF comparison\n",
    "    rtf_data = successful_df.groupby('model')['rtf'].mean().sort_values()\n",
    "    axes[0].barh(rtf_data.index, rtf_data.values, color='skyblue')\n",
    "    axes[0].axvline(x=1, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    axes[0].set_xlabel('RTF', fontsize=12)\n",
    "    axes[0].set_title('Real-Time Factor (RTF)', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Generation time comparison\n",
    "    time_data = successful_df.groupby('model')['generation_time'].mean().sort_values()\n",
    "    axes[1].barh(time_data.index, time_data.values, color='lightcoral')\n",
    "    axes[1].set_xlabel('Seconds', fontsize=12)\n",
    "    axes[1].set_title('Average Generation Time', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de √âxitos y Fallos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty:\n",
    "    # Contar √©xitos y fallos por modelo\n",
    "    success_counts = metrics_df.groupby(['model', 'success']).size().unstack(fill_value=0)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TASA DE √âXITO POR MODELO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calcular porcentajes\n",
    "    if True in success_counts.columns:\n",
    "        success_counts['Success Rate (%)'] = (success_counts.get(True, 0) / \n",
    "                                               success_counts.sum(axis=1) * 100).round(2)\n",
    "    \n",
    "    display(success_counts)\n",
    "    \n",
    "    # Mostrar errores si existen\n",
    "    failed_df = metrics_df[metrics_df['success'] == False]\n",
    "    if not failed_df.empty:\n",
    "        print(\"\\n‚ö†Ô∏è ERRORES ENCONTRADOS:\")\n",
    "        print(\"=\" * 60)\n",
    "        for _, row in failed_df.iterrows():\n",
    "            print(f\"Model: {row['model']}\")\n",
    "            print(f\"Error: {row.get('error', 'Unknown')}\")\n",
    "            print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumen y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty and not successful_df.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RESUMEN DE EVALUACI√ìN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Modelo m√°s r√°pido (menor RTF)\n",
    "    fastest_model = successful_df.groupby('model')['rtf'].mean().idxmin()\n",
    "    fastest_rtf = successful_df.groupby('model')['rtf'].mean().min()\n",
    "    \n",
    "    # Modelo m√°s consistente (menor std en RTF)\n",
    "    most_consistent = successful_df.groupby('model')['rtf'].std().idxmin()\n",
    "    consistency_std = successful_df.groupby('model')['rtf'].std().min()\n",
    "    \n",
    "    print(f\"\\nüèÜ Modelo m√°s r√°pido: {fastest_model}\")\n",
    "    print(f\"   RTF promedio: {fastest_rtf:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nüìä Modelo m√°s consistente: {most_consistent}\")\n",
    "    print(f\"   Desviaci√≥n est√°ndar RTF: {consistency_std:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RECOMENDACIONES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if fastest_rtf < 1:\n",
    "        print(f\"‚úì {fastest_model} puede generar audio en tiempo real\")\n",
    "        print(\"  ‚Üí Ideal para aplicaciones interactivas\")\n",
    "    \n",
    "    print(f\"\\n‚úì Para m√°xima velocidad: usar {fastest_model}\")\n",
    "    print(f\"‚úì Para resultados predecibles: usar {most_consistent}\")\n",
    "    \n",
    "    # An√°lisis de calidad vs velocidad\n",
    "    print(\"\\nüìù NOTA: Este an√°lisis solo considera m√©tricas de rendimiento (velocidad).\")\n",
    "    print(\"   Para evaluar calidad de audio, considera:\")\n",
    "    print(\"   - Speaker Similarity (Resemblyzer, ECAPA-TDNN)\")\n",
    "    print(\"   - Audio Quality (PESQ, STOI)\")\n",
    "    print(\"   - Evaluaci√≥n subjetiva (MOS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_df.empty:\n",
    "    # Exportar a CSV\n",
    "    output_csv = results_dir / 'metrics_summary.csv'\n",
    "    successful_df.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úì Resumen exportado a: {output_csv}\")\n",
    "    \n",
    "    # Exportar estad√≠sticas agregadas\n",
    "    if not successful_df.empty:\n",
    "        summary_stats = successful_df.groupby('model').agg({\n",
    "            'rtf': ['mean', 'std', 'min', 'max'],\n",
    "            'generation_time': ['mean', 'std', 'min', 'max'],\n",
    "            'audio_duration': ['mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        output_stats = results_dir / 'metrics_statistics.csv'\n",
    "        summary_stats.to_csv(output_stats)\n",
    "        print(f\"‚úì Estad√≠sticas exportadas a: {output_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notas\n",
    "\n",
    "Este notebook analiza las m√©tricas b√°sicas de rendimiento. Para una evaluaci√≥n completa, considera implementar:\n",
    "\n",
    "1. **Speaker Similarity**: Usar Resemblyzer o SpeechBrain ECAPA-TDNN para medir similitud de voz\n",
    "2. **Audio Quality**: Usar PESQ y STOI para evaluar calidad perceptual\n",
    "3. **Subjective Evaluation**: MOS (Mean Opinion Score) con evaluadores humanos\n",
    "\n",
    "Para m√°s informaci√≥n, consulta el README del proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
